{
"cells": [
 {
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "<h1 align=\"center\"><font color=\"0066FF\" size=110>Eigenvalue problems I: Introduction and Jacobi Method</font></h1>\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "\n",
  "\n"
  ]
 },
{
 "cell_type": "code",
 "metadata": {},
  "outputs": [],
 "execution_count": 1,

 "source": [
  "import numpy as np\n",
  "import scipy as sp\n",
  "import scipy.linalg\n",
  "import matplotlib\n",
  "from IPython.html.widgets import interact\n",
  "from IPython.display import Image, YouTubeVideo\n",
  "try:\n",
  "    %matplotlib inline\n",
  "except:\n",
  "    # not in notebook\n",
  "    pass\n",
  "LECTURE = False\n",
  "if LECTURE:\n",
  "    size = 20\n",
  "    matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
  "    matplotlib.rcParams['axes.labelsize'] = size\n",
  "    matplotlib.rcParams['axes.titlesize'] = size\n",
  "    matplotlib.rcParams['xtick.labelsize'] = size * 0.6\n",
  "    matplotlib.rcParams['ytick.labelsize'] = size * 0.6\n",
  "import matplotlib.pyplot as plt\n",
  "\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "# Learning Outcomes\n",
  "\n",
  "-   Define an eigenvalue problems\n",
  "-   Give two examples of applications from the physical world involving the solution of eigenvalue problems.\n",
  "-   Solve a simple eigenvalue problem analytically.\n",
  "-   Explain in your own words the Jacobi algorithm for eigenvalue problems.\n",
  "\n",
  "# Introduction\n",
  "\n",
  "Along with the differential equations we have covered to date, there is another class of problems that are important in science and engineering: eigenvalue problems. They are often associated with vibrations and they also have some interesting and unique solvers which bring together a number of the technique and methods we have covered.\n",
  "\n",
  "## Eigenvalue problems and resonance problems\n",
  "\n",
  "All mechanical structures have *natural frequencies* at which they like to vibrate. Those natural frequencies depends on the distribution of mass in the structure, as well as the stiffness of the elements that make up that structure. If the structure is excited at any of these natural frequencies, it will resonate: a small excitation will produce a very large response. This response can be large enough to to lead to a break up of the structure.\n",
  "\n",
  "For example, a continuous tonal sound at the same frequency as one of the natural frequencies of a glass [can make it break](https://www.youtube.com/watch?v%3D17tqXgvCN0E).\n",
  "\n"
  ]
 },
{
 "cell_type": "code",
 "metadata": {},
  "outputs": [],
 "execution_count": 1,

 "source": [
  "YouTubeVideo('17tqXgvCN0E')\n",
  "\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "It is therefore crucial to design engineering structures so that the natural resonances will not be excited. Failure to predict such resonances can lead to catastrophic collapse, as in the well known case of the [Tacoma Narrows Bridge Collapse](https://www.youtube.com/watch?v%3DXggxeuFDaDU)\n",
  "\n"
  ]
 },
{
 "cell_type": "code",
 "metadata": {},
  "outputs": [],
 "execution_count": 1,

 "source": [
  "YouTubeVideo('XggxeuFDaDU')\n",
  "\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "Another recent exemple is provided by [the Millenium bridge](https://www.youtube.com/watch?v%3DeAXVa__XWZ8), which had to be closed shortly after its inauguration because of a resonance problem.\n",
  "\n"
  ]
 },
{
 "cell_type": "code",
 "metadata": {},
  "outputs": [],
 "execution_count": 1,

 "source": [
  "YouTubeVideo('eAXVa__XWZ8')\n",
  "\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "Note that, for each natural frequency, the response follows a particular shape called the *mode shape*. The natural frequencies correspond to the *eigenvalues* of the system, while the mode shapes correspond to *eigenvectors*.\n",
  "\n",
  "## Eigenvalue problems and wave phenomena\n",
  "\n",
  "### Acoustics\n",
  "\n",
  "In addition to vibration problems, eigenvalue problems are ubiquitous in acoustics or optical phenomena. For example, when sound waves propagate through a pipe, they take the form of particular duct modes, corresonding to the eigenvectors of the Helmhotz equation. Similarly, sound field in a closed room fluctates according to the acoustic modes of the room. The sound quality of a concert hall therefore depends on the acoustic modes of the room.\n",
  "\n",
  "![img](royal-albert-hall.jpg \"Picture of the Royal Albert Hall, showing circular pads hanging off the roof to improve the sound quality.\")\n",
  "\n",
  "### Optics\n",
  "\n",
  "Other examples of eigenvalue problems in optics include the scattering of light by the wings of butterflies, the feathers of peacocks and the surface of opals. This produces colors that are due to structural coloring rather than pigmentation. The stunning colors they produce depend on the refraction index of these optical systems, which can be obtained by solving an eigenvalue problem.\n",
  "\n",
  "![img](opals.jpg \"Opals exhibit stunning colors without pigments but via structural coloring.\")\n",
  "\n",
  "![img](peacock.jpg \"Peacocks feathers are another example of structural coloring.\")\n",
  "\n",
  "# Eigenvalue problem definition\n",
  "\n",
  "The genrealised eigenvalue problem for two $N \\times N$ matrices $A$ and $B$ is to find a set of *eigenvalues* $\\lambda$ and *eigenvectors* $x \\neq 0$ such that\n",
  "\n",
  "\\begin{equation}\n",
  "A x = \\lambda B x.\n",
  "\\end{equation}\n",
  "\n",
  "If $B$ is the identity matrix, this equation reduces to the eigenvalue problem:\n",
  "\n",
  "\\begin{equation}\n",
  "A x = \\lambda x.\n",
  "\\end{equation}\n",
  "\n",
  "We will focus on the latter equation as techniques to solve the generalised problem are an extension of solving the reduced problem and can be found in the literature [⁣1]\n",
  "\n",
  "# Analytical solution\n",
  "\n",
  "## Theory\n",
  "\n",
  "We can solve the eigenvalue problem as follows. If there exists $x \\neq 0$ such that\n",
  "\n",
  "\\begin{equation}\n",
  "A x = \\lambda x,\n",
  "\\end{equation}\n",
  "\n",
  "then\n",
  "\n",
  "\\begin{equation}\n",
  "(A - \\lambda I)x = 0.\n",
  "\\end{equation}\n",
  "\n",
  "This must be true for each eigenvalue-eigenvector pair. Since $x$ must be non-zero, the above equation has a solution if and only if\n",
  "\n",
  "\\begin{equation}\n",
  "\\det(A - \\lambda I) = 0.\n",
  "\\end{equation}\n",
  "\n",
  "This leads to a polynomial equation of order $n$, called the *characteristic polynomial*, whose solutions are the eigenvalues of matrix $A$. It is then generally easy to find an eigenvector for each of the eigenvalue $\\lambda$ by finding a solution $x$ satisfying $(A - \\lambda I)x= 0$\n",
  "\n",
  "## Example\n",
  "\n",
  "Let\n",
  "\n",
  "\\begin{equation}\n",
  "A = %\n",
  "\\begin{pmatrix}\n",
  " 3 & -1 &  0 \\\\\n",
  "-1 &  2 & -1 \\\\\n",
  " 0 & -1 &  3\n",
  "\\end{pmatrix}.\n",
  "\\end{equation}\n",
  "\n",
  "We can find the eigenvalues by solving\n",
  "\n",
  "\\begin{equation}\n",
  "\\det(A - \\lambda I) = %\n",
  "\\begin{vmatrix}\n",
  " 3 - \\lambda & -1 &  0 \\\\\n",
  "-1 &  2 - \\lambda& -1 \\\\\n",
  " 0 & -1 &  3 - \\lambda\n",
  "\\end{vmatrix} = 0.\n",
  "\\end{equation}\n",
  "\n",
  "This gives\n",
  "\n",
  "\\begin{equation}\n",
  "(3-\\lambda)\n",
  "\\begin{vmatrix}\n",
  " 2 - \\lambda & -1 \\\\\n",
  "-1 &  3 - \\lambda\n",
  "\\end{vmatrix}\n",
  "- (-1)\n",
  "\\begin{vmatrix}\n",
  " -1 &  -1 \\\\\n",
  "  0 &  3 - \\lambda\n",
  "\\end{vmatrix}\n",
  "+ (0)\n",
  "\\begin{vmatrix}\n",
  " -1 &  2 - \\lambda \\\\\n",
  "  0 &  -1\n",
  "\\end{vmatrix} = 0.\n",
  "\\end{equation}\n",
  "\n",
  "Thus we have the characteristic polynomial\n",
  "\n",
  "\\begin{align}\n",
  "(3 - \\lambda)(2-\\lambda)(3-\\lambda) - (3 - \\lambda) - 1(3 - \\lambda) &= 0  \\\\\n",
  "-\\lambda^3 + 8 \\lambda^2 - 21\\lambda + 18 - 3 + \\lambda - 3 + \\lambda &= 0 \\\\\n",
  "-\\lambda^3 + 8 \\lambda^2 - 19 \\lambda + 12 = 0.\n",
  "\\end{align}\n",
  "\n",
  "This can be solved to give the solutions\n",
  "\n",
  "\\begin{align}\n",
  "\\lambda_1 &= 1, & \\lambda_2 &= 4, & \\lambda_3 &= 3.\n",
  "\\end{align}\n",
  "\n",
  "Solving $(A - \\lambda I)x=0$ for each of these eigenvalues yields the corresponding eigenvectors, for example\n",
  "\n",
  "\\begin{align}\n",
  "x_1 &= \\begin{pmatrix} 1 \\\\  2 \\\\ 1 \\end{pmatrix}, &\n",
  "x_2 &= \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}, &\n",
  "x_3 &= \\begin{pmatrix} 1 \\\\  0 \\\\ -1 \\end{pmatrix}.\n",
  "\\end{align}\n",
  "\n",
  "Note that the eigenvectors are not unique and can be multiplied by arbitrary constants.\n",
  "\n",
  "Alternatively, we can also use the SymPy module in Python (or other symoblic mathematical software such as Mathematica or Maple).\n",
  "\n"
  ]
 },
{
 "cell_type": "code",
 "metadata": {},
  "outputs": [],
 "execution_count": 1,

 "source": [
  "import sympy as sy\n",
  "L = sy.symbols('L')\n",
  "M = sy.Matrix([[3 - L, -1, 0],\n",
  "               [-1, 2 - L, -1],\n",
  "               [0, -1, 3 - L]])\n",
  "print 'Characteristic Polynomial: P(L) = ', M.det()\n",
  "\n",
  "print 'Eigenvalues: ', sy.solve(M.det())\n",
  "\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "But writing out the characteristic polynomial and solving it is generally not a good way to solve as the matrix gets larger.\n",
  "\n",
  "Clearly if the matrix was of the form\n",
  "\n",
  "\\begin{equation}\n",
  "D =\n",
  "\\begin{pmatrix}\n",
  " a &  0 &  0 \\\\\n",
  " 0 &  b &  0 \\\\\n",
  " 0 &  0 &  c\n",
  "\\end{pmatrix},\n",
  "\\end{equation}\n",
  "\n",
  "then we would find the eigenvalues trivially since we would have\n",
  "\n",
  "\\begin{align}\n",
  "\\det(D - \\lambda I) &= 0 \\\\\n",
  "\\begin{vmatrix}\n",
  " a - \\lambda &  0 &  0 \\\\\n",
  " 0 &  b - \\lambda &  0 \\\\\n",
  " 0 &  0 &  c - \\lambda\n",
  "\\end{vmatrix} &= 0 \\\\\n",
  "(a - \\lambda)(b - \\lambda)(c - \\lambda) &= 0,\n",
  "\\end{align}\n",
  "\n",
  "so the the eigenvalues are $a$, $b$, $c$. Thus, our first strategy for finding the eigenvalues will be to attempt to transform the matrix into a diagonal matrix.\n",
  "\n",
  "If $P$ is the transformation matrix that transforms $A$ into a diagonal matrix $D$ , then\n",
  "\n",
  "\\begin{equation}\n",
  "D = P^{-1} A P,\n",
  "\\end{equation}\n",
  "\n",
  "which can be easily shown by noting that the transformation matrix transforms a vector $x$ expressed in the original basis, into a vector $x'$ expressed in the new basis, as\n",
  "\n",
  "\\begin{equation}\n",
  "x = P x'.\n",
  "\\end{equation}\n",
  "\n",
  "It can also be shown easily from the above equation that the columns of the matrix transformation give the coordinates of the eigenvalues in the original basis.\n",
  "\n",
  "# Numerical solution: the Jacobi Method fo Eigenvalues\n",
  "\n",
  "## Algorithm\n",
  "\n",
  "This method is a fairly robust way to extract all of the eigenvalues and eigenvectors of a symmetric matrix. Whilst it is probably only appropriate to use for matrices up to 20 by 20, the principles of how this method operates underpin a number of more complicated methods that can be used more generally to find all of the eigenvalues of a matrix (assuming that finding such eigenvalues is actually a well-posed / stable / sensible problem).\n",
  "\n",
  "The method is based on a series of rotations, called Jacobi or [Givens rotations](https://en.wikipedia.org/wiki/Givens_rotation), which are chosen to eliminate off-diagonal elements while preserving the eigenvalues. Whilst successive rotations will undo previous et zeros, the off-diagonal elements get smaller until eventually we are left with a diagonal matrix. By accumulating products of the transformations as we proceed we obtain the eigenvectors of the matrix. See for example [⁣2, 3] for additional details.\n",
  "\n",
  "Consider the transformation matrix $P(p, q, \\theta)$ of the form\n",
  "\n",
  "\\begin{equation}\n",
  "P =\n",
  "\\begin{pmatrix}\n",
  "1 & & & & & & & & \\\\\n",
  " & \\ddots & & & & & & & \\\\\n",
  " & & 1 & & & & & & \\\\\n",
  " & & & c & &  s & & & \\\\\n",
  " & & & & \\large 1 & & & & \\\\\n",
  " & & & -s & & c & & & \\\\\n",
  " & & & & & & 1 & & \\\\\n",
  " & & & & & &  & \\ddots & \\\\\n",
  " & & & & & &  & & 1\n",
  "\\end{pmatrix},\n",
  "\\end{equation}\n",
  "\n",
  "where all diagonal eleents are unity apart from two elements $c$ in rows $p$ and $q$, and all off-diagonal elements are zero apart from the elements $s$ and $-s$ (in rows and columns $p$ and $q$). Thus, if $e_p$ and $e_q$ are the $p$<sup>th</sup> and $q$<sup>th</sup> vectors of an orthonormal basis, and if\n",
  "\n",
  "\\begin{align}\n",
  "c &= \\cos \\theta, & s &= \\sin \\theta,\n",
  "\\end{align}\n",
  "\n",
  "then $P$ represents a rotation of angle $\\theta$ in the (oriented) $(e_p, e_q)$ plane, which leaves all other basis vectors unchanged.\n",
  "\n",
  "Applying this transformation matrix to the symmetric matrix $A$ yields\n",
  "\n",
  "\\begin{equation}\n",
  "\\widetilde A = P^T A P,\n",
  "\\end{equation}\n",
  "\n",
  "which is also a symmetric matrix, and whose eigenvalues are the same as those of $A$. Furthermore, this operation preserves the Frobenius norm, i.e.\n",
  "\n",
  "\\begin{equation}\n",
  "\\sum_{i,j} \\widetilde A_{ij}^2 = \\sum_{i,j} A_{ij}^2,\n",
  "\\end{equation}\n",
  "\n",
  "so our rotations are not changing the \"size\" of $A$, only redistributing the coefficients.\n",
  "\n",
  "Most importantly, for each column $q$ in $A$, a matrix $P(p, q, \\theta)$ can be defined such that $A_{p,q} = 0$ (see e.g. [⁣2, 3] or [Wikipedia](https://en.wikipedia.org/wiki/Jacobi_eigenvalue_algorithm)). A desirable side effect is that\n",
  "\n",
  "\\begin{equation}\n",
  "\\widetilde A_{qq}^2 + \\widetilde A_{pp}^2 \\geq A_{qq}^2 + A_{pp}^2,\n",
  "\\end{equation}\n",
  "\n",
  "i.e. the diagonal elements increase in magnitude when going from $A$ to $\\widetilde A$. This effect is especially large if row $p$ is chosen such that $A_{pq}$ is the largest coefficient in the $q$<sup>th</sup> column of $A$. Thus, we can rotate our matrix $A$ in such a way that the large off-diagonal coefficients become zero, while the on-diagonal coefficients increase in magnitude.\n",
  "\n",
  "If we carry on this process iteratively, selecting $p$, $q$ such that $A_{pq}$ is the largest off-diagonal element, we will eventually obtain a matrix $D$ that is diagonal to machine precision:\n",
  "\n",
  "\\begin{equation}\n",
  "D = P^T A P,\n",
  "\\end{equation}\n",
  "\n",
  "where\n",
  "\n",
  "\\begin{equation}\n",
  "P = P_1 P_2 P_3 \\cdots P_m,\n",
  "\\end{equation}\n",
  "\n",
  "is the product of the successive Jacobi rotation matrices.\n",
  "\n",
  "As explained in the previous section, the diagonal coefficients in $D$ represent the eigenvalues, while the columns of $P$ give the coordinates of the eigenvectors in our original basis.\n",
  "\n",
  "Sample code for this algorithm can be found in [⁣3].\n",
  "\n",
  "## Implementation\n",
  "\n"
  ]
 },
{
 "cell_type": "code",
 "metadata": {},
  "outputs": [],
 "execution_count": 1,

 "source": [
  "def maxelem(a):\n",
  "    \"\"\"Return (amax, k, l), the value and indices of the off-diagonal\n",
  "    element in 2D array a\n",
  "    \"\"\"\n",
  "    n = len(a)\n",
  "    amax = 0.0\n",
  "    for i in range(n-1):\n",
  "        for j in range(i+1,n):\n",
  "            if abs(a[i,j]) >= amax:\n",
  "                amax = abs(a[i,j])\n",
  "                k = i\n",
  "                l = j\n",
  "    return amax,k,l\n",
  "\n",
  "\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "Define a function for rotating the matrix in place:\n",
  "\n"
  ]
 },
{
 "cell_type": "code",
 "metadata": {},
  "outputs": [],
 "execution_count": 1,

 "source": [
  "def rotate(a, p, k, l):\n",
  "    \"\"\"Rotate input matrix a in place to make a[k,l] = 0, and update the\n",
  "    transformation matrix p\n",
  "    \"\"\"\n",
  "    n = len(a)\n",
  "    aDiff = a[l,l] - a[k,k]\n",
  "    if abs(a[k,l]) < abs(aDiff)*1.0e-36:\n",
  "        t = a[k,l]/aDiff\n",
  "    else:\n",
  "        phi = aDiff/(2.0*a[k,l])\n",
  "        t = 1.0/(abs(phi) + np.sqrt(phi**2 + 1.0))\n",
  "        if phi < 0.0:\n",
  "            t = -t\n",
  "    c = 1.0/np.sqrt(t**2 + 1.0); s = t*c\n",
  "    tau = s/(1.0 + c)\n",
  "    temp = a[k,l]\n",
  "    a[k,l] = 0.0\n",
  "    a[k,k] = a[k,k] - t*temp\n",
  "    a[l,l] = a[l,l] + t*temp\n",
  "    # Case of i < k\n",
  "    for i in range(k):\n",
  "        temp = a[i,k]\n",
  "        a[i,k] = temp - s*(a[i,l] + tau*temp)\n",
  "        a[i,l] = a[i,l] + s*(temp - tau*a[i,l])\n",
  "    # Case of k < i < l\n",
  "    for i in range(k+1,l):\n",
  "        temp = a[k,i]\n",
  "        a[k,i] = temp - s*(a[i,l] + tau*a[k,i])\n",
  "        a[i,l] = a[i,l] + s*(temp - tau*a[i,l])\n",
  "    # Case of i > l\n",
  "    for i in range(l+1,n):\n",
  "        temp = a[k,i]\n",
  "        a[k,i] = temp - s*(a[l,i] + tau*temp)\n",
  "        a[l,i] = a[l,i] + s*(temp - tau*a[l,i])\n",
  "    # Update transformation matrix\n",
  "    for i in range(n):\n",
  "        temp = p[i,k]\n",
  "        p[i,k] = temp - s*(p[i,l] + tau*p[i,k])\n",
  "        p[i,l] = p[i,l] + s*(temp - tau*p[i,l])\n",
  "\n",
  "\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "Implement the Jacobi eigenvalue algorithm\n",
  "\n"
  ]
 },
{
 "cell_type": "code",
 "metadata": {},
  "outputs": [],
 "execution_count": 1,

 "source": [
  "def jacobi_eig(a, tol=1.0e-9): # Jacobi method\n",
  "    \"\"\" lambda, x = jacobi_eig(a, tol=1.0e-9).\n",
  "\n",
  "    Solution of std. eigenvalue problem [a]{x} = lambda {x}\n",
  "    by Jacobi's method. Returns eigenvalues in vector {lam}\n",
  "    and the eigenvectors as columns of matrix [x].\n",
  "    \"\"\"\n",
  "    n = len(a)\n",
  "    # Set limit on number of rotations\n",
  "    nbrot_max = 5*(n**2)\n",
  "    # Initialize transformation matrix\n",
  "    p = np.identity(n)*1.0\n",
  "    # Jacobi rotation loop\n",
  "    for i in range(nbrot_max):\n",
  "        amax, k, l = maxelem(a)\n",
  "        if amax < tol:\n",
  "            return np.diagonal(a),p\n",
  "        rotate(a, p, k,l)\n",
  "        # # Extra debug\n",
  "        # print 'Step:', i\n",
  "        # print a\n",
  "        # print'----------------'\n",
  "    print 'Jacobi method did not converge'\n",
  "\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "## Testing\n",
  "\n",
  "Test our implementation:\n",
  "\n"
  ]
 },
{
 "cell_type": "code",
 "metadata": {},
  "outputs": [],
 "execution_count": 1,

 "source": [
  "#Create full matrix\n",
  "A= np.array([[3.,-1,0], [-1,2,-1] , [0 , -1 , 3]])\n",
  "#A= np.array([[8.,-1,3,-1], [-1,6,2,0] , [3 , 2 , 9, 1], [-1, 0, 1, 7]])\n",
  "w,v = jacobi_eig(A,tol =  1.0e-9)\n",
  "\n",
  "# Now sort them into order with argsort\n",
  "idx = w.argsort()\n",
  "w = w[idx]\n",
  "v = v[:,idx]\n",
  "\n",
  "# Normalize the eigenvectors so the first coordinate equals 1\n",
  "v = v / v[0].reshape(1, -1)\n",
  "print 'Eigenvalues:\\n', w\n",
  "print ''\n",
  "print 'Eigenvectors:\\n', v\n",
  "\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "# Self study\n",
  "\n",
  "Solve the eigenvalue problem for matrix $A$ below using an analytical approach (e.g. with SymPy), and using the Jacobi eigenvalue method.\n",
  "\n",
  "\\begin{equation}\n",
  "A =\n",
  "\\begin{pmatrix}\n",
  "8 & -1 & 3 & -1 \\\\\n",
  "-1 & 6 & 2 & 0 \\\\\n",
  "3 & 2 & 9 & 1 \\\\\n",
  "-1 & 0 & 1 & 7\n",
  "\\end{pmatrix}\n",
  "\\end{equation}\n",
  "\n",
  "# Conclusions\n",
  "\n",
  "-   Eigenvalue problems: given a matrix $A$, find scalars $\\lambda$, called eigenvalues, and non-zero vectors $x$, called eigenvectors, such that $Ax = \\lambda x$.\n",
  "-   Give two examples of applications from the physical world involving the solution of eigenvalue problems: vibrations (bridge dynamics) and the design of concert halls (acoustic modes).\n",
  "-   Solve a simple eigenvalue problem analytically: calculate the characteristic polynomial $\\det(A - \\lambda I)$ and find the eigenvalues, then find an eigenvector for each eigenvalue by solving $(A - \\lambda I)x = 0$.\n",
  "-   Explain in your own words the Jacobi algorithm for eigenvalue problems\n",
  "\n",
  "# References\n",
  "\n",
  "1.  G. H. Golub and C. F. Van Loan, Matrix Computations, John Hopkins University Press; third edition (1996) ISBN-10:0801854148\n",
  "2.  W. H. Press, S.A. Teukolsky et al, Numerical Recipes in C.\n",
  "3.  J. Kiusalaas, Numerical Methods in Engineering with Python, Cambridge University Press (2010).\n",
  "\n"
  ]
 }
],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}