{
"cells": [
 {
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "<h1 align=\"center\"><font color=\"0066FF\" size=110>Eigenvalue problems III: Advanced Numerical Methods</font></h1>\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "\n",
  "\n"
  ]
 },
{
 "cell_type": "code",
 "metadata": {},
  "outputs": [],
 "execution_count": 1,

 "source": [
  "import numpy as np\n",
  "import scipy as sp\n",
  "import scipy.linalg\n",
  "\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "# Learning Outcomes\n",
  "\n",
  "By the end of this lecture, you should be able to\n",
  "-   List three alternatives to the Jacobi eigenvalue method for solving an eigenvalue problem.\n",
  "-   Discuss the principle behind each method and its main advantage.\n",
  "\n",
  "# Introduction\n",
  "\n",
  "The the previous lectures on eigenvalue problems we introduced and used the Jacobi eigenvalue method, which uses successive rotation matrices to diagonalise the input matrix $A$. The Jacobi eigenvalue method requires the input matrix $A$ to be symmetric. In this lecture, we will discuss alternative numerical methods, applicable to generic matrices, including:\n",
  "-   the inverse power method\n",
  "-   the Krylov subspace method\n",
  "-   the QR method\n",
  "\n",
  "# Inverse Power method: finding the smallest eigenvalue of a symmetric matrix\n",
  "\n",
  "## Theory\n",
  "\n",
  "Consider an eigenvalue problem\n",
  "\n",
  "\\begin{equation}\n",
  "\\mathbf A \\mathbf x = \\lambda \\mathbf x,\n",
  "\\end{equation}\n",
  "\n",
  "where $A$ is an $n \\times n$ matrix.\n",
  "\n",
  "We will use an iterative approach similar to the one introduced in lecture 01:\n",
  "1.  take an initial guess\n",
  "2.  solve the iterative problem to obtain the solution\n",
  "3.  set that solution as your new guess and go to step 2 until the solution converges.\n",
  "\n",
  "Here we define the iterative problem as follows. Given a **unit vector** $\\mathbf v$, Find the vector $\\mathbf z$ such that\n",
  "\n",
  "\\begin{equation}\n",
  "\\mathbf A \\mathbf z = \\mathbf v.\n",
  "\\end{equation}\n",
  "\n",
  "Compute $|\\mathbf z|$ and let $\\mathbf v = \\mathbf z / |\\mathbf z|$ for the next iteration. Stop when the magnitude of $|\\mathbf z|$ has converged.\n",
  "\n",
  "At the conclusion of the procedure, $|\\mathbf z| = \\pm 1/\\lambda_1$, where $\\lambda_1$ is the smallest eigenvalue of $\\mathbf A$. Note that we have assumed here that $\\lambda_1$ is real. If it is complex, then $|\\mathbf z| = 1/|\\lambda_1|$.\n",
  "\n",
  "### Why does this work?\n",
  "\n",
  "If you expand $\\mathbf v$ and $\\mathbf z$ along the basis of eigenvectors $\\mathbf x_1, \\mathbf x_2, \\cdots$, it is easy to show that the component of $\\mathbf x_1$ increases in $\\mathbf z$ compared to $\\mathbf v$, whereas the components of other eigenvectors $\\mathbf x_i$, $i > 1$, decrease [⁣1]. The decrease along these eigenvectors $\\mathbf x_i$ is of the form $\\lambda_1 / \\lambda_i$ which is strictly smaller than 1 since $\\lambda_1$ is the smallest eigenvalue. This means that the vector $\\mathbf z$ becomes more aligned with $\\mathbf x_1$ with each iteration.\n",
  "\n",
  "In the limit, we get\n",
  "\n",
  "\\begin{equation}\n",
  "\\mathbf z = \\frac{1}{\\lambda_1} \\mathbf x_1,\n",
  "\\end{equation}\n",
  "\n",
  "where $\\mathbf x_1$ is a unit eigenvector and where $\\lambda_1$ is the smallest eigenvalue of $\\mathbf A$.\n",
  "\n",
  "## Implementation\n",
  "\n"
  ]
 },
{
 "cell_type": "code",
 "metadata": {},
  "outputs": [],
 "execution_count": 1,

 "source": [
  "def norm(x):\n",
  "    \"\"\"Return norm of input vector\"\"\"\n",
  "    return np.sqrt(np.sum(x**2))\n",
  "\n",
  "\n",
  "def invpower_eig(A, tol=1e-9, maxiter=1000):\n",
  "    \"\"\"Return the smallest eigenvalue and the cooresponding\n",
  "    eigenvector, using the Inverse Power method\"\"\"\n",
  "\n",
  "    def iterate(z):\n",
  "        \"\"\"Return new guess vector\"\"\"\n",
  "        # Normalise z to get v\n",
  "        v = z / norm(z)\n",
  "        # Solve Az = v\n",
  "        return sp.linalg.solve(A, v)\n",
  "\n",
  "    def has_converged(zold, znew):\n",
  "        return abs(norm(zold) - norm(znew)) < tol\n",
  "\n",
  "    n = A.shape[0]\n",
  "    zold = np.ones(n)\n",
  "    znew = iterate(zold)\n",
  "    count = 0\n",
  "    while not has_converged(zold, znew) or count > maxiter :\n",
  "        zold = znew  # store znew\n",
  "        znew = iterate(zold)\n",
  "        count += 1\n",
  "    # Get the sign of the eigenvalue\n",
  "    z = np.dot(A, znew)\n",
  "    sign = np.sign(z[0] * znew[0])\n",
  "    # Return the eigenvalue\n",
  "    lambda1 = sign * 1.0 / norm(znew)\n",
  "    return lambda1, znew\n",
  "\n",
  "\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "## Testing\n",
  "\n"
  ]
 },
{
 "cell_type": "code",
 "metadata": {},
  "outputs": [],
 "execution_count": 1,

 "source": [
  "A= np.array([[3.,-1,0], [-1,2,-1] , [0 , -1 , 3]])\n",
  "#A= np.array([[8.,-1,3,-1], [-1,6,2,0] , [3 , 2 , 9, 1], [-1, 0, 1, 7]])\n",
  "lambda1, x1 = invpower_eig(A,tol=1.0e-9)\n",
  "print 'smallest eigenvalue = ', lambda1\n",
  "print 'eigenvector = ', x1 / x1[0]\n",
  "\n"
  ]
 },
{
 "cell_type": "markdown",
 "metadata": {},
  
 "source": [
  "We find $\\lambda_1 = 1$ and $x_1 = \\pmatrix{1 \\\\ 2 \\\\ 1}$ as expected (see lecture 08).\n",
  "\n",
  "### Self study\n",
  "\n",
  "Modify the power method code above to return the smallest eigenvalue and eigenvector when the eigenvalue is complex. Use the following matrix as an example:\n",
  "\n",
  "\\begin{equation}\n",
  "A =\n",
  "\\begin{pmatrix}\n",
  "  3 & -9 \\\\\n",
  "  4 & -3\n",
  "\\end{pmatrix}\n",
  "\\end{equation}\n",
  "\n",
  "Solution: the eigenvalues are $\\lambda_1 = \\pm 3 \\sqrt{3} i$.\n",
  "\n",
  "## Discussion\n",
  "\n",
  "Recall that convergence depends on the ratio $\\lambda_1 / \\lambda_i$ ($i > 1$). Convergence is therefore slow if $\\lambda_i$ is close to $\\lambda_1$. However we can shift all the eigenvalues by an arbitrary constant $s$, so the ration becomes $(\\lambda_1 - s)/ (\\lambda_i - s)$ which can help speed up the convergence if we have a good estimate $s$ for $\\lambda_1$ [⁣1]. An important application of this is to find all the eigenvectors of a matrix once the eigenvalues are known. The  QR Method, introduced below, is a popular method for computing all the eigenvalues of a matrix.\n",
  "\n",
  "A great advantage of this approach is that it does not transform the input matrix $\\mathbf A$. It is well suited to finding the eigenvalues of a sparse matrix.\n",
  "\n",
  "There is a similar method called the *Power method* to compute the largest eigenvalue of $\\matrix A$. The main differences with the inverse Power method is that it solves the system\n",
  "\n",
  "\\begin{equation}\n",
  "\\mathbf z = \\mathbf A \\mathbf v\n",
  "\\end{equation}\n",
  "\n",
  "at each iteration, and that $|\\mathbf z|$ converges to $\\pm \\lambda_n$, and $\\mathbf v$ converges to $\\mathbf x_n$.\n",
  "\n",
  "# Krylov subspace method\n",
  "\n",
  "The inverse Power method can be generalised to obtain the first $n$ eigenvalues and eigenvectors of a matrix. This is the purpose of [Arnoldi's iteration method](https://en.wikipedia.org/wiki/Arnoldi_iteration), which starts from a guess vector $\\mathbf v$ and generates the vectors $(\\mathbf A^{i} \\mathbf v)_{0 \\leq i < n}$. These vectors form a space called a *Krylov subspace*, and orthonormalising these vectors provides a good estimate for the eigenvectors.\n",
  "\n",
  "# QR iteration for eigenvalues\n",
  "\n",
  "A classic method for finding all the eigenvalues of a symmetric matrix is the QR method. It is based on the $QR$ factorization of a symmetric matrix $A$, where $Q$ is an orthogonal matrix ($Q^TQ=1$) and $R$ is an upper diagonal matrix.\n",
  "\n",
  "It is easy to show that if $A = QR$, then $A_1 = RQ$ has the same eigenvalues as $A$. If we carry out the QR decomposition of $A_n = Q_n R_n$, and define $A_{n+1} = R_n Q_n$, we can obtain a series of matrices $(A_n)_{n \\geq 1}$ based on $A$.\n",
  "\n",
  "For a symmetric matrix $A$, the list $(A_n)_{n \\geq 1}$ leads to upper diagonal matrices, whose diagonal coefficients tend towards the eigenvalues of $A$.\n",
  "\n",
  "# Scipy\n",
  "\n",
  "The Scipy linalg module `scipy.linalg`, and its sparse version `scipy.sparse.linalg` implement several functions for eigenvalue problems including:\n",
  "-   `sp.linalg.eig` : returns eigenvalues and eigenvectors of a generic (real or complex) matrix\n",
  "-   `sp.linalg.eigh` : returns eigenvalues and eigenvectors of a symmetric or Hermitian matrix.\n",
  "-   `sp.sparse.linalg.eigs` : returns the first $k$ eigenvalues and eigenvectors of a sparse matrix (Scipy version 0.15.1).\n",
  "\n",
  "Matlab has an additional function `eigs` that returns the first $k$ eigenvalues and eigenvectors of a general matrix.\n",
  "\n",
  "# Conclusions\n",
  "\n",
  "-   Numerous algorithms are available for computing the eigenvalues and eigenvectors of a matrix. Ideally the user should employ an algorithm that is optimal for the matrix at hand. For example, it should exploit the symmetry, sparsity or bandedness of the matrix.\n",
  "-   The QR algorithm can be used to find the eigenvalues of a symmetric matrix.\n",
  "-   The inverse Power method can be used to find the eigenvectors of a matrix given the eigenvalues, or to find the smallest eigenvalue and its eigenvector.\n",
  "-   Kryolv subspace methods can be be used to find the first $k$ eigenvectors of a general matrix.\n",
  "\n",
  "# References\n",
  "\n",
  "1.  J. Kiusalaas, Numerical Methods in Engineering with Python, Cambridge University Press (2010).\n",
  "\n"
  ]
 }
],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}